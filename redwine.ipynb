{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV as gridsearchcv\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, auc, roc_curve\n",
    "import sklearn.metrics\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import AdaBoostClassifier as ada\n",
    "from sklearn.svm import SVC as svc\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Move to correct folder for server.  Can remove before sending\n",
    "# os.chdir('/home/poblivsig/Dropbox/horses2')\n",
    "os.chdir('/home/poblivsig/Dropbox/cancer_and_phishing')\n",
    "\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Open the pre-processed csv\n",
    "df = pd.read_csv('data/winequality-red.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Get info about wine\n",
    "print(f'Shape\\n\\n{df.shape}')\n",
    "print(f'Columns\\n\\n{df.columns}')\n",
    "print(f'dtypes\\n\\n{df.dtypes}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(f'Description\\n\\n{df.describe()}')\n",
    "print(f'Info:\\n{df.info}')\n",
    "print(f'Check out the sample: {df.sample(n=1)}')\n",
    "pd.set_option('display.max_columns', 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the different quality values\n",
    "sns.countplot(df['quality'],\n",
    "              palette='Blues',\n",
    "              label=\"Quality Count\", )\n",
    "plt.plot()\n",
    "plt.savefig('data/charts/bc_diag_countplot.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create mini-histograms for each attribute\n",
    "df.hist(bins=10,\n",
    "        figsize=(10, 8))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the amount of correlation between each column and the quality\n",
    "corrs = df.corr()\n",
    "corr_quality = corrs['quality']\n",
    "print('Amount of correlation (Pearsons r) for each column:')\n",
    "print(corr_quality.sort_values(ascending=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a scatterplot matrix with a Kernel Density estimation on the diagonal\n",
    "scatter_matrix = pd.plotting.scatter_matrix(df, diagonal='kde', cmap='Blues', figsize=(12, 12))\n",
    "\n",
    "#May need to offset label when rotating to prevent overlap of figure\n",
    "[scat.get_yaxis().set_label_coords(-0.9, 0.4) for scat in scatter_matrix.reshape(-1)]\n",
    "\n",
    "# Rotate all of the column names\n",
    "[scat.xaxis.label.set_rotation(45) for scat in scatter_matrix.reshape(-1)]\n",
    "[scat.yaxis.label.set_rotation(0) for scat in scatter_matrix.reshape(-1)]\n",
    "\n",
    "# Remove all of the markings and numbers along the axes\n",
    "[scat.set_xticks(()) for scat in scatter_matrix.reshape(-1)]\n",
    "[scat.set_yticks(()) for scat in scatter_matrix.reshape(-1)]\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col_names = ['fixed acidity',\n",
    "             'volatile acidity',\n",
    "             'citric acid',\n",
    "             'residual sugar',\n",
    "             'chlorides',\n",
    "             'free sulfur dioxide',\n",
    "             'total sulfur dioxide',\n",
    "             'density',\n",
    "             'pH',\n",
    "             'sulphates',\n",
    "             'alcohol',\n",
    "             'quality']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Create color map\n",
    "colormap = sns.diverging_palette(220,\n",
    "                                 10,\n",
    "                                 as_cmap=True)\n",
    "\n",
    "# Create Heat Map, including annotations\n",
    "# Put the floating point numbers in the map\n",
    "sns.heatmap(corrs,\n",
    "            cmap='Blues',\n",
    "            fmt=\".2f\",\n",
    "            annot=True)\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    col_names,\n",
    "    horizontalalignment='right',\n",
    "    rotation=45)\n",
    "\n",
    "ax.set_yticklabels(col_names)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################\n",
    "# Build boxplots for the most correlated against the different quality levels (3)\n",
    "\n",
    "## Alcohol\n",
    "plot = sns.boxplot(x='quality',\n",
    "                   y='alcohol',\n",
    "                   data=df,\n",
    "                   showfliers=False)\n",
    "plot.set_title('Quality vs. Alcohol')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Volatile acidity\n",
    "plot = sns.boxplot(x='quality',\n",
    "                   y='volatile acidity',\n",
    "                   data=df,\n",
    "                   showfliers=False)\n",
    "plot.set_title('Quality vs. Volatile Acidity')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Sulphates\n",
    "plot = sns.boxplot(x='quality',\n",
    "                   y='sulphates',\n",
    "                   data=df,\n",
    "                   showfliers=False)\n",
    "plot.set_title('Quality vs. Sulphates')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Citric Acid\n",
    "plot = sns.boxplot(x='quality',\n",
    "                   y='citric acid',\n",
    "                   data=df,\n",
    "                   showfliers=False)\n",
    "plot.set_title('Quality vs. Fixed Acidity')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Residual Sugar\n",
    "plot = sns.boxplot(x='quality',\n",
    "                   y='residual sugar',\n",
    "                   data=df,\n",
    "                   showfliers=False)\n",
    "plot.set_title('Quality vs. Residual Sugar')\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Split the data up.\n",
    "y = df['quality']\n",
    "X = df.drop('quality', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=14)\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the features (attributes)\n",
    "scaler = RobustScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# print(cross_val_score(decisiontreeclassifier(criterion = 'gini', random_state = 0), x_train, y_train, cv=5))\n",
    "def gridsearch(estimator, param_grid, cv, scoring_metric):\n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "    clf = gridsearchcv(estimator=estimator,\n",
    "                       param_grid=param_grid,\n",
    "                       n_jobs=-1,\n",
    "                       cv=cv,\n",
    "                       return_train_score=True,\n",
    "                       scoring=scorer,\n",
    "                       verbose=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ### Output the results\n",
    "    print(f'Best parameters: {clf.best_params_}')\n",
    "    print(f'Best score: {clf.best_score_}')\n",
    "    best_estimate = clf.best_estimator_\n",
    "    print(best_estimate)\n",
    "\n",
    "    ## Now we have found the best parameters, use them...\n",
    "    best_estimate.fit(X_train,y_train)\n",
    "\n",
    "    predictor = best_estimate.predict(X_train)\n",
    "    mse = mean_squared_error(predictor, y_train)\n",
    "    r2 = r2_score(predictor, y_train)\n",
    "    print(f'Training Mean Square Error: {mse:.2f}')\n",
    "    print(f'Training R2: {r2:.2f}')\n",
    "\n",
    "    y_predictor = best_estimate.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predictor)\n",
    "    r2 = r2_score(y_test, y_predictor)\n",
    "    print(f'Testing Mean Square Error: {mse:.2f}')\n",
    "    print(f'Testing R2: {r2:.2f}')\n",
    "    print('blah')\n",
    "\n",
    "    return best_estimate, y_predictor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cm_and_class_rep(X_test, y_test, y_predictor, best_estimate):\n",
    "    confusion_matrix(y_test, y_predictor)\n",
    "    cm = plot_confusion_matrix(best_estimate,\n",
    "                               X_test,\n",
    "                               y_test,\n",
    "                               cmap=plt.cm.Blues,\n",
    "                               normalize='true' )\n",
    "    plt.show(cm)\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_predictor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_learning_curve(estimator_1,\n",
    "                        estimator_1_name,\n",
    "                        estimator_2,\n",
    "                        estimator_2_name,\n",
    "                        estimator_3,\n",
    "                        estimator_3_name,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        cv,\n",
    "                        train_max,\n",
    "                        title):\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    ###################################\n",
    "    # Do the 1st curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_1,\n",
    "                                 X_train,\n",
    "                                 y_train,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_1_name,\n",
    "             color='blue')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_1_name,\n",
    "             color='cornflowerblue')\n",
    "\n",
    "\n",
    "    ###################################\n",
    "    # Do the 2nd curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_2,\n",
    "                                 X_train,\n",
    "                                 y_train,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_2_name,\n",
    "             color='green')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_2_name,\n",
    "             color='springgreen')\n",
    "\n",
    "    ###################################\n",
    "    # Do the 3rd curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_3,\n",
    "                                 X_train,\n",
    "                                 y_train,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_3_name,\n",
    "             color='red')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_3_name,\n",
    "             color='lightcoral')\n",
    "\n",
    "    # Do the final plots\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Set Size'), plt.ylabel('Accuracy'), plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('data/charts/wine_learning_curve.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw Validation Curve\n",
    "def draw_validation_curve(estimator, X_train, y_train, cv, param_name, param_range, title, xlabel):\n",
    "\n",
    "    train_scores, test_scores = validation_curve(estimator,\n",
    "                                                 X_train,\n",
    "                                                 y_train,\n",
    "                                                 param_name=param_name,\n",
    "                                                 param_range=param_range,\n",
    "                                                 cv=cv,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs=-1)\n",
    "\n",
    "    # Mean from the scores for the training set\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "    # Mean from the scores for the test set\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    # Create plot for training\n",
    "    plt.plot(param_range,\n",
    "             train_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"blue\")\n",
    "\n",
    "    # Create plot for testing\n",
    "    plt.plot(param_range,\n",
    "             test_mean,\n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"red\")\n",
    "\n",
    "    # Build plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/charts/wine_validation_curve.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do Decision Tree stuff...\n",
    "\n",
    "# Do a grid search for the Decision Tree\n",
    "dt_criterion = ['gini', 'entropy']\n",
    "dt_max_depth = [9] #[count for count in range(0, 30)]\n",
    "dt_class_weight = [None, 'balanced']\n",
    "print(f'dt_max_depth = {dt_max_depth}')\n",
    "param_grid = dict(criterion=dt_criterion,\n",
    "                  max_depth=dt_max_depth,\n",
    "                  class_weight=dt_class_weight)\n",
    "\n",
    "best_estimate, y_predictor = gridsearch(estimator=dtc(),\n",
    "                                        param_grid=param_grid,\n",
    "                                        cv=8,\n",
    "                                        scoring_metric='recall')\n",
    "\n",
    "cm_and_class_rep(X_test, y_test, y_predictor, best_estimate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decision Tree learning curve\n",
    "\n",
    "## TODO: Grab these values from best_estimate above\n",
    "tuned_criterion = 'entropy'\n",
    "tuned_max_depth=9\n",
    "tuned_class_weight='balanced'\n",
    "\n",
    "# Best depth\n",
    "draw_learning_curve(dtc(criterion=tuned_criterion,\n",
    "                        max_depth=10,\n",
    "                        class_weight=tuned_class_weight),\n",
    "                    'max_depth = 10',\n",
    "                    dtc(criterion=tuned_criterion,\n",
    "                        max_depth=17,\n",
    "                        class_weight=tuned_class_weight),\n",
    "                    'max_depth = 17',\n",
    "                    dtc(criterion=tuned_criterion,\n",
    "                        max_depth=24,\n",
    "                        class_weight=tuned_class_weight),\n",
    "                    'max_depth = 24',\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=8,\n",
    "                    train_max=900,\n",
    "                    title = 'DT Red Wine Learning Curve Training Set Size vs Accuracy using Various Max Depths')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw Decision Tree Validation Curve\n",
    "draw_validation_curve(dtc(criterion=tuned_criterion,\n",
    "                          class_weight=tuned_class_weight),\n",
    "                      X_train,\n",
    "                      y_train,\n",
    "                      cv=8,\n",
    "                      param_name='max_depth',\n",
    "                      param_range=np.arange(0,80),\n",
    "                      title='DT Red Wine Validation Curve for Maximum Depth',\n",
    "                      xlabel='Maximum Depth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do KNN stuff\n",
    "# Set the parameters by cross-validation\n",
    "MAX_NEIGHBORS=19\n",
    "n_neighbors=[layers for layers in range(1, MAX_NEIGHBORS)]\n",
    "metric=['manhattan', 'euclidean', 'chebyshev']\n",
    "weights=['uniform', 'distance']\n",
    "algorithm=['kd_tree', 'ball_tree']\n",
    "param_grid = dict(n_neighbors=n_neighbors,\n",
    "                  metric=metric,\n",
    "                  weights=weights,\n",
    "                  algorithm=algorithm)\n",
    "best_estimate, y_predictor = gridsearch(estimator=knn(),\n",
    "                                        param_grid=param_grid,\n",
    "                                        cv=8,\n",
    "                                        scoring_metric='precision')\n",
    "\n",
    "cm_and_class_rep(X_test, y_test, y_predictor, best_estimate)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KNN learning curve\n",
    "tuned_n_neighbors = 13\n",
    "tuned_metric = 'manhattan'\n",
    "tuned_weights = 'distance'\n",
    "tuned_algorithm = 'kd_tree'\n",
    "\n",
    "draw_learning_curve(knn(n_neighbors=3,\n",
    "                        metric=tuned_metric,\n",
    "                        weights=tuned_weights,\n",
    "                        algorithm=tuned_algorithm),\n",
    "                    'Nearest Neighbors = 2',\n",
    "                    knn(n_neighbors=4,\n",
    "                        metric=tuned_metric,\n",
    "                        weights=tuned_weights,\n",
    "                        algorithm=tuned_algorithm),\n",
    "                    'Nearest Neighbors = 3',\n",
    "                    knn(n_neighbors=5,\n",
    "                        metric=tuned_metric,\n",
    "                        weights=tuned_weights,\n",
    "                        algorithm=tuned_algorithm),\n",
    "                    'Nearest Neighbors = 4',\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=8,\n",
    "                    train_max=500,\n",
    "                    title = 'KNN Red Wine Learning Curve Training Set Size vs Nearest Neighbors')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuned_n_neighbors = 13\n",
    "tuned_metric = 'manhattan'\n",
    "tuned_weights = 'distance'\n",
    "tuned_algorithm = 'kd_tree'\n",
    "\n",
    "# Draw KNN Validation Curve\n",
    "draw_validation_curve(knn(n_neighbors=tuned_n_neighbors,\n",
    "                          metric=tuned_metric,\n",
    "                          weights=tuned_weights,\n",
    "                          algorithm=tuned_algorithm),\n",
    "                      X_train,\n",
    "                      y_train,\n",
    "                      cv=7,\n",
    "                      param_name='n_neighbors',\n",
    "                      param_range=np.arange(0, MAX_NEIGHBORS),\n",
    "                      title='KNN Red Wine Validation for Nearest Neighbors',\n",
    "                      xlabel='Number of Nearest Neighbors')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do Boosting stuff\n",
    "# Set the parameters by cross-validation\n",
    "ada_learning_rate = [(2**x)/100 for x in range(7)]\n",
    "# ada_n_estimators = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "ada_n_estimators = [20, 30, 40]\n",
    "# N.B. These hyper-parameters are from previous decision tree tuning.\n",
    "ada_dt = dtc(criterion=tuned_criterion,\n",
    "             max_depth=tuned_max_depth,\n",
    "             class_weight=tuned_class_weight)I\n",
    "param_grid = dict(learning_rate=ada_learning_rate,\n",
    "                  n_estimators=ada_n_estimators)\n",
    "best_estimate, y_predictor = gridsearch(estimator=ada(ada_dt),\n",
    "                                        param_grid=param_grid,\n",
    "                                        cv=10,\n",
    "                                        scoring_metric='recall')\n",
    "\n",
    "cm_and_class_rep(X_test, y_test, y_predictor, best_estimate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Boosting learning curve\n",
    "\n",
    "## Grab these values from best_estimate above\n",
    "tuned_learning_rate = 0.16\n",
    "tuned_n_estimators = 80\n",
    "\n",
    "draw_learning_curve(ada(ada_dt,\n",
    "                        learning_rate=tuned_learning_rate,\n",
    "                        n_estimators=20),\n",
    "                    'no. of estimators = 20',\n",
    "                    ada(ada_dt,\n",
    "                        learning_rate=tuned_learning_rate,\n",
    "                        n_estimators=80),\n",
    "                    'no. of estimators = 80',\n",
    "                    ada(ada_dt,\n",
    "                        learning_rate=tuned_learning_rate,\n",
    "                        n_estimators=140),\n",
    "                    'no. of estimators = 140',\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=8,\n",
    "                    train_max=500,\n",
    "                    title = 'AdaBoost Red Wine Learning Curve Training Set Size vs No. of Estimators')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw Boosting Validation Curve\n",
    "draw_validation_curve(ada(ada_dt,\n",
    "                          learning_rate=tuned_learning_rate,\n",
    "                          n_estimators=tuned_n_estimators),\n",
    "                      X_train,\n",
    "                      y_train,\n",
    "                      cv=8,\n",
    "                      param_name='n_estimators',\n",
    "                      param_range=ada_n_estimators,\n",
    "                      title='AdaBoost Red Wine Validation Curve for No. of Estimators',\n",
    "                      xlabel='Number of Estimators')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do SVM stuff\n",
    "# Set the parameters by cross-validation\n",
    "svc_C = np.arange(0.001, 2.5, 0.5)\n",
    "svc_tol = np.arange(1e-8, 1e-1, 0.05)\n",
    "svc_kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "# svc_gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "svc_gamma = [0.1]\n",
    "param_grid = dict(C=svc_C,\n",
    "                  tol=svc_tol,\n",
    "                  kernel=svc_kernel,\n",
    "                  gamma = svc_gamma)\n",
    "best_estimate, y_predictor = gridsearch(estimator=svc(),\n",
    "                                        param_grid=param_grid,\n",
    "                                        cv=8,\n",
    "                                        scoring_metric='precision')\n",
    "\n",
    "cm_and_class_rep(X_test, y_test, y_predictor, best_estimate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tuned_C = 1.501\n",
    "tuned_tol = 1 ** -8\n",
    "tuned_kernel = 'rbf'\n",
    "tuned_gamma = 1\n",
    "\n",
    "# SVM learning curve\n",
    "draw_learning_curve(svc(C=tuned_C,\n",
    "                        tol=tuned_tol,\n",
    "                        kernel=tuned_kernel,\n",
    "                        gamma=0.1),\n",
    "                        'gamma = 0.1',\n",
    "                    svc(C=tuned_C,\n",
    "                        tol=tuned_tol,\n",
    "                        kernel=tuned_kernel,\n",
    "                        gamma=1),\n",
    "                     'gamma = 1',\n",
    "                    svc(C=tuned_C,\n",
    "                        tol=tuned_tol,\n",
    "                        kernel=tuned_kernel,\n",
    "                        gamma=10),\n",
    "                    'gamma = 10',\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=8,\n",
    "                    train_max=500,\n",
    "                    title = 'SVM Red Wine Learning Curve Training Set Sizes vs Gamma')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw SVM Validation Curve\n",
    "draw_validation_curve(svc(C=tuned_C,\n",
    "                          tol=tuned_tol,\n",
    "                          kernel=tuned_kernel,\n",
    "                          gamma=1),\n",
    "                      X_train,\n",
    "                      y_train,\n",
    "                      cv=8,\n",
    "                      param_name='gamma',\n",
    "                      param_range=[10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                      title='SVM Red Wine Validation for Gamma',\n",
    "                      xlabel='Gamma')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do MLP stuff...\n",
    "MAX_LAYER_SIZE = 20\n",
    "hidden_layer_sizes = [layers for layers in range(1, MAX_LAYER_SIZE)]\n",
    "activation = ['tanh', 'relu']\n",
    "max_iter = [500, 1000, 1500, 2000]\n",
    "alpha = 10.0 ** -np.arange(1,7)\n",
    "learning_rate = ['constant', 'adaptive']\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                  activation=activation,\n",
    "                  max_iter=max_iter,\n",
    "                  alpha=alpha,\n",
    "                  learning_rate=learning_rate)\n",
    "\n",
    "best_estimate, y_predictor = gridsearch(estimator=mlp(),\n",
    "                                        param_grid=param_grid,\n",
    "                                        cv=8,\n",
    "                                        scoring_metric='precision')\n",
    "\n",
    "cm_and_class_rep(X_test, y_test, y_predictor, best_estimate)\n",
    "\n",
    "\n",
    "# MLP learning curve\n",
    "tuned_hidden_layer_sizes = 19\n",
    "tuned_activation = 'tanh'\n",
    "tuned_max_iter = 1000\n",
    "tuned_alpha = 1e-06\n",
    "tuned_learning_rate = 'constant'\n",
    "\n",
    "draw_learning_curve(mlp(hidden_layer_sizes=4,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 14',\n",
    "                    mlp(hidden_layer_sizes=tuned_hidden_layer_sizes,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 19',\n",
    "                    mlp(hidden_layer_sizes=34,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 24',\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    cv=8,\n",
    "                    train_max=500,\n",
    "                    title = 'MLP Phishing Learning Curve Training Set Size vs. hidden layer size')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw MLP Validation Curve\n",
    "draw_validation_curve(mlp(hidden_layer_sizes=tuned_hidden_layer_sizes,\n",
    "                          activation=tuned_activation,\n",
    "                          max_iter=tuned_max_iter,\n",
    "                          alpha=tuned_alpha,\n",
    "                          learning_rate=tuned_learning_rate),\n",
    "                      X_train,\n",
    "                      y_train,\n",
    "                      cv=8,\n",
    "                      param_name='hidden_layer_sizes',\n",
    "                      param_range=np.arange(0, MAX_LAYER_SIZE),\n",
    "                      title='MLP Phishing Validation Curve for Hidden Layer Sizes',\n",
    "                      xlabel='Hidden Layer Sizes')\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ed9a20bf",
   "language": "python",
   "display_name": "PyCharm (cancer_and_phishing)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}